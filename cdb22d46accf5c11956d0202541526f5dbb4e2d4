{
  "comments": [
    {
      "unresolved": true,
      "key": {
        "uuid": "7c762249_c0684019",
        "filename": "/COMMIT_MSG",
        "patchSetId": 6
      },
      "lineNbr": 10,
      "author": {
        "id": 1006192
      },
      "writtenOn": "2020-10-29T15:59:21Z",
      "side": 1,
      "message": "It isn\u0027t really a \"maximising\" thing but rather a requirement. If you have the two containers in HA being placed into the same EC2 instance on the same AZ, then it is clearly not HA at all, because the failure of the node OR the AZ would cause an outage.",
      "range": {
        "startLine": 10,
        "startChar": 9,
        "endLine": 10,
        "endChar": 17
      },
      "revId": "cdb22d46accf5c11956d0202541526f5dbb4e2d4",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "47fa8b24_65eae785",
        "filename": "/COMMIT_MSG",
        "patchSetId": 6
      },
      "lineNbr": 10,
      "author": {
        "id": 1072905
      },
      "writtenOn": "2020-10-29T16:48:09Z",
      "side": 1,
      "message": "you\u0027re right.\nThe thing is that at the moment the entire deployment of aws-gerrit happens on one *AZ* only, and it is not possible to configure otherwise, unless we start introducing the concept of subnet*s* into it.\n\nI think it should be done and perhaps we need additional issues to capture this need.\n\nAs far as this change is concerned, perhaps I can phrase it as \"...in order to increase Gerrit reliability...\"\n\nWDYT?",
      "parentUuid": "7c762249_c0684019",
      "range": {
        "startLine": 10,
        "startChar": 9,
        "endLine": 10,
        "endChar": 17
      },
      "revId": "cdb22d46accf5c11956d0202541526f5dbb4e2d4",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "4a391ecd_866ec026",
        "filename": "/COMMIT_MSG",
        "patchSetId": 6
      },
      "lineNbr": 10,
      "author": {
        "id": 1006192
      },
      "writtenOn": "2020-10-29T17:16:35Z",
      "side": 1,
      "message": "Also in a single AZ, having both masters on the same EC2 instance isn\u0027t assuring any HA. It is more for \"assuring\" rather than increasing or maximising.",
      "parentUuid": "47fa8b24_65eae785",
      "range": {
        "startLine": 10,
        "startChar": 9,
        "endLine": 10,
        "endChar": 17
      },
      "revId": "cdb22d46accf5c11956d0202541526f5dbb4e2d4",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "d347ad67_b3c0315e",
        "filename": "/COMMIT_MSG",
        "patchSetId": 6
      },
      "lineNbr": 10,
      "author": {
        "id": 1072905
      },
      "writtenOn": "2020-10-29T17:57:33Z",
      "side": 1,
      "message": "Ack",
      "parentUuid": "4a391ecd_866ec026",
      "range": {
        "startLine": 10,
        "startChar": 9,
        "endLine": 10,
        "endChar": 17
      },
      "revId": "cdb22d46accf5c11956d0202541526f5dbb4e2d4",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "1b614d56_7b915ce5",
        "filename": "/COMMIT_MSG",
        "patchSetId": 6
      },
      "lineNbr": 10,
      "author": {
        "id": 1006192
      },
      "writtenOn": "2020-10-30T23:37:03Z",
      "side": 1,
      "message": "Did you forget to apply the changes?",
      "parentUuid": "d347ad67_b3c0315e",
      "range": {
        "startLine": 10,
        "startChar": 9,
        "endLine": 10,
        "endChar": 17
      },
      "revId": "cdb22d46accf5c11956d0202541526f5dbb4e2d4",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "b1e4eb12_933c013a",
        "filename": "/COMMIT_MSG",
        "patchSetId": 6
      },
      "lineNbr": 10,
      "author": {
        "id": 1072905
      },
      "writtenOn": "2020-11-02T10:01:08Z",
      "side": 1,
      "message": "Done",
      "parentUuid": "1b614d56_7b915ce5",
      "range": {
        "startLine": 10,
        "startChar": 9,
        "endLine": 10,
        "endChar": 17
      },
      "revId": "cdb22d46accf5c11956d0202541526f5dbb4e2d4",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "7756bed4_8f0c53ba",
        "filename": "/COMMIT_MSG",
        "patchSetId": 6
      },
      "lineNbr": 14,
      "author": {
        "id": 1006192
      },
      "writtenOn": "2020-10-29T15:59:21Z",
      "side": 1,
      "message": "Isn\u0027t that more for the LB vs. master node problem rather than having true HA with the separation of the two EC2 instances?",
      "range": {
        "startLine": 12,
        "startChar": 0,
        "endLine": 14,
        "endChar": 68
      },
      "revId": "cdb22d46accf5c11956d0202541526f5dbb4e2d4",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "8489d864_cea2c6f1",
        "filename": "/COMMIT_MSG",
        "patchSetId": 6
      },
      "lineNbr": 14,
      "author": {
        "id": 1072905
      },
      "writtenOn": "2020-10-29T16:48:09Z",
      "side": 1,
      "message": "I think it is both: separating ha-proxies from masters obtains two gains\n- Increasing HA\n- Allowing ha-proxies to always talk to both masters rather than just the one deployed on a different instance.",
      "parentUuid": "7756bed4_8f0c53ba",
      "range": {
        "startLine": 12,
        "startChar": 0,
        "endLine": 14,
        "endChar": 68
      },
      "revId": "cdb22d46accf5c11956d0202541526f5dbb4e2d4",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "0c552d04_19698d80",
        "filename": "/COMMIT_MSG",
        "patchSetId": 6
      },
      "lineNbr": 14,
      "author": {
        "id": 1006192
      },
      "writtenOn": "2020-10-29T17:16:35Z",
      "side": 1,
      "message": "Gotcha, but your phrase is again about the L4 networking issue. I agree more with your two gains, because the L4 networking issue has been already resolved by  Change-Id: I38ad5d2.\n\nCan you align the commit message with your explanation? Which makes a lot of sense to me :-)",
      "parentUuid": "8489d864_cea2c6f1",
      "range": {
        "startLine": 12,
        "startChar": 0,
        "endLine": 14,
        "endChar": 68
      },
      "revId": "cdb22d46accf5c11956d0202541526f5dbb4e2d4",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "e0df1d6d_112a62f4",
        "filename": "/COMMIT_MSG",
        "patchSetId": 6
      },
      "lineNbr": 14,
      "author": {
        "id": 1072905
      },
      "writtenOn": "2020-10-29T17:57:33Z",
      "side": 1,
      "message": "Ack",
      "parentUuid": "0c552d04_19698d80",
      "range": {
        "startLine": 12,
        "startChar": 0,
        "endLine": 14,
        "endChar": 68
      },
      "revId": "cdb22d46accf5c11956d0202541526f5dbb4e2d4",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "b1f40581_c12e30a8",
        "filename": "/COMMIT_MSG",
        "patchSetId": 6
      },
      "lineNbr": 14,
      "author": {
        "id": 1006192
      },
      "writtenOn": "2020-11-02T12:39:48Z",
      "side": 1,
      "message": "Still forgot to push? I still see the L4 networking thing: this change is about separating masters, correct? The HAProxy cluster has been already put onto a separate ASG, so it should be fixed right now.",
      "parentUuid": "e0df1d6d_112a62f4",
      "range": {
        "startLine": 12,
        "startChar": 0,
        "endLine": 14,
        "endChar": 68
      },
      "revId": "cdb22d46accf5c11956d0202541526f5dbb4e2d4",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "3374e204_624d9885",
        "filename": "/COMMIT_MSG",
        "patchSetId": 6
      },
      "lineNbr": 14,
      "author": {
        "id": 1072905
      },
      "writtenOn": "2020-11-02T13:01:04Z",
      "side": 1,
      "message": "Yes, I am sorry luca, I got two changes mixed up: this is not at all about ha-proxies.\n\nit should *actually* be fixed now ðŸ˜Š",
      "parentUuid": "b1f40581_c12e30a8",
      "range": {
        "startLine": 12,
        "startChar": 0,
        "endLine": 14,
        "endChar": 68
      },
      "revId": "cdb22d46accf5c11956d0202541526f5dbb4e2d4",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "d1059a2d_af2dfe61",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 6
      },
      "lineNbr": 0,
      "author": {
        "id": 1006192
      },
      "writtenOn": "2020-10-29T15:59:21Z",
      "side": 1,
      "message": "It looks like we are forcing the cluster to be fixed at 1x EC2 instance, instead of having a parametrised capacity.",
      "revId": "cdb22d46accf5c11956d0202541526f5dbb4e2d4",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "dcf4c395_3d9899be",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 6
      },
      "lineNbr": 0,
      "author": {
        "id": 1072905
      },
      "writtenOn": "2020-10-29T16:48:09Z",
      "side": 1,
      "message": "I think the \"desired number of instances\" made sense before because we had to accommodate multiple instances in one ASG only.\n\nBut this is not the case anymore, we have different ASGs for different components and master1 and master2 will always take one and exactly one ec2 instance.\nMasters cannot elastically scale out, that\u0027s why we explicitly configure master1 and master2 from the setup.env down to the cf templates.\n\nWhat would mean to have a desired cluster of 3 for the master1 ASG?",
      "parentUuid": "d1059a2d_af2dfe61",
      "revId": "cdb22d46accf5c11956d0202541526f5dbb4e2d4",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "a3878f5e_12bdd773",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 6
      },
      "lineNbr": 0,
      "author": {
        "id": 1006192
      },
      "writtenOn": "2020-10-29T17:16:35Z",
      "side": 1,
      "message": "If that was the case, why we didn\u0027t have a fixed number of EC2 instances before?\nAlso, the ASG is for the EC2 instances of the ECS cluster, and not for *exclusively* the master node.\n\nI don\u0027t believe that removing a parameter that we have now for then introducing it again in the next change is a good thing, but rather a regression IMHO.",
      "parentUuid": "dcf4c395_3d9899be",
      "revId": "cdb22d46accf5c11956d0202541526f5dbb4e2d4",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "ab51059f_284bceeb",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 6
      },
      "lineNbr": 0,
      "author": {
        "id": 1072905
      },
      "writtenOn": "2020-10-29T17:57:33Z",
      "side": 1,
      "message": "\u003e If that was the case, why we didn\u0027t have a fixed number of EC2 instances before?\n\nBecause depending on the spec of Gerrit you might have wanted/needed more EC2 instances to host them:\n- If you deployed masters with very low ram/cpu needs, for example, then perhaps 2 instances would have been enough (to host masters,haproxy,slaves), whilst if you needed more resources for Gerrit, you might have wanted more ec2 instances to allow them to be allocated.\n\n\u003e Also, the ASG is for the EC2 instances of the ECS cluster, and not for *exclusively* the master node.\n\nNot anymore, we have 3 ASGs in the cluster at the moment (master, replica, haproxy) and the \"desired number of instances\" is not a property of the ECS cluster, but rather a property of the ASG.\n\nThere are 3 different ASG that could potentially be tuned with [min-desired-max], one for each ASG.\n\n* haproxy -\u003e this should go to a 2-2-n (as per your point in the previous CR, I will raise the relevant change)\n* master -\u003e this should be 1-1-1, always, because masters cannot scale.\n* replica -\u003e this should be 1-1-1 until we introduce EFS that allow them to scale.\n\nWDYT?",
      "parentUuid": "a3878f5e_12bdd773",
      "revId": "cdb22d46accf5c11956d0202541526f5dbb4e2d4",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "7372e1b8_b8275fe1",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 6
      },
      "lineNbr": 0,
      "author": {
        "id": 1006192
      },
      "writtenOn": "2020-10-29T20:35:48Z",
      "side": 1,
      "message": "\u003e \u003e If that was the case, why we didn\u0027t have a fixed number of EC2 instances before?\n\u003e \n\u003e Because depending on the spec of Gerrit you might have wanted/needed more EC2 instances to host them:\n\u003e - If you deployed masters with very low ram/cpu needs, for example, then perhaps 2 instances would have been enough (to host masters,haproxy,slaves), whilst if you needed more resources for Gerrit, you might have wanted more ec2 instances to allow them to be allocated.\n\nI don\u0027t get it. If you had two masters, how allocating 6 EC2 would have given more resources?\n\nI believe before it was more forward thinking, allowing the next iterations of this recipe to have more horsepower. Remember that we still need to add additional components:\n- Git daemon\n- Git/SSH\n- JGit GC node\n\n\u003e \u003e Also, the ASG is for the EC2 instances of the ECS cluster, and not for *exclusively* the master node.\n\u003e \n\u003e Not anymore, we have 3 ASGs in the cluster at the moment (master, replica, haproxy) and the \"desired number of instances\" is not a property of the ECS cluster, but rather a property of the ASG.\n\nAren\u0027t the 3 ASGs part of the ECS cluster, just with different labels?\nWe can of course have 2 settings now:\n- Desired number of masters\u0027 EC2 instances\n- Desired number of WLB EC2 instances\n\nDepending on how many many containers you want in each ASG, you may want different number of instances.\n\n\u003e There are 3 different ASG that could potentially be tuned with [min-desired-max], one for each ASG.\n\u003e \n\u003e * haproxy -\u003e this should go to a 2-2-n (as per your point in the previous CR, I will raise the relevant change)\n\nN should be configurable\n\n\u003e * master -\u003e this should be 1-1-1, always, because masters cannot scale.\n\nNope, you would need extra horsepower for additional components, as I mentioned.\nAlso, when you do blue/green deployments, I imagine that ECS would do:\na. Create the new task\nb. Wait for the task to be healthy\nc. Kill the old task\n\nIf you have only 1 fixed EC2 instance, it may actually fail to do the upgrades with blue/green deployments.\n\n\u003e * replica -\u003e this should be 1-1-1 until we introduce EFS that allow them to scale.\n\nWhy? Replicas are intended to be scaled up, as they are stateless. This is exactly the point where we want to have the maximum number of instances as a parameter.\n\nWe do have already the parameter for allowing them to scale up, I can understand the point of making it 3 parameters, which is fair. I don\u0027t get the point of hardcoding the maximum values, as they are not hardcoded now.",
      "parentUuid": "ab51059f_284bceeb",
      "revId": "cdb22d46accf5c11956d0202541526f5dbb4e2d4",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "eb656d7e_2a17c3bb",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 6
      },
      "lineNbr": 0,
      "author": {
        "id": 1072905
      },
      "writtenOn": "2020-10-30T09:44:30Z",
      "side": 1,
      "message": "\u003e \u003e \u003e If that was the case, why we didn\u0027t have a fixed number of EC2 instances before?\n\u003e \u003e \n\u003e \u003e Because depending on the spec of Gerrit you might have wanted/needed more EC2 instances to host them:\n\u003e \u003e - If you deployed masters with very low ram/cpu needs, for example, then perhaps 2 instances would have been enough (to host masters,haproxy,slaves), whilst if you needed more resources for Gerrit, you might have wanted more ec2 instances to allow them to be allocated.\n\u003e \n\u003e I don\u0027t get it. If you had two masters, how allocating 6 EC2 would have given more resources?\n\n\n\nImagine these are the specs defined for your deployment\n\n* Master Service *\nGerrit:\n- Memory: 6Gb\n- Cpu: 1024 (1 vCPU)\n\n* Replica Service *\nGerrit:\n- Memory: 6Gb\n- Cpu: 1024 (1 vCPU)\n\nGit daemon:\n- Memory: 512Mb\n- Cpu: 256 (0.25 vCPU)\n\nGit SSH:\n- Memory: 512Mb\n- Cpu: 256 (0.25 vCPU)\n\n* Loadbalancer service *\n\nha-proxy:\n- Memory: 2Gb\n- Cpu: 1024 (1 vCPU)\n\nTotal Memory needed: (6Gb x 2) + (6Gb + 512Mb + 512Mb) + 2Gb \u003d 19Gb\nTotal CPU needed:   (1024 x 2) + (1024 + 256 + 256)    + 1024 \u003d 4608 (~ 5vCPUs)\n\nNow, let\u0027s assume your you have chosen a cluster EC2 instance type of\n\nm4.xlarge: 4 vCPUs, 16 Gb RAM\n\nYou will need a desired number of instances of _at least_ \u00272\u0027: if you had \u00271\u0027 you wouldn\u0027t have room for 19Gb and 5 vCPUs.\n\nIf you changed your gerrit specs to be higher however, you might need a cluster of 3, and so on.\n\nThe number of instances in the cluster is a function of _needed resources_, not just of _number of instances_.\n\n\n\n\n\n\u003e \n\u003e I believe before it was more forward thinking, allowing the next iterations of this recipe to have more horsepower. Remember that we still need to add additional components:\n\u003e - Git daemon\n\u003e - Git/SSH\n\n\n\n^^^ These are already part of the replica service\n\n\n\n\n\u003e - JGit GC node\n\n\n\nNot sure this one will need to be a component: it is an operational task and it could be modelled as a ECS daemon[1], or a step function, or SSM, or a lambda, etc.\n\n\n\n\u003e \n\u003e \u003e \u003e Also, the ASG is for the EC2 instances of the ECS cluster, and not for *exclusively* the master node.\n\u003e \u003e \n\u003e \u003e Not anymore, we have 3 ASGs in the cluster at the moment (master, replica, haproxy) and the \"desired number of instances\" is not a property of the ECS cluster, but rather a property of the ASG.\n\u003e \n\u003e Aren\u0027t the 3 ASGs part of the ECS cluster, just with different labels?\n\nYes, they are 3 ASGs _registered_ to the same ECS cluster.\n\n\u003e We can of course have 2 settings now:\n\u003e - Desired number of masters\u0027 EC2 instances\n\u003e - Desired number of WLB EC2 instances\n\n\n\nWhat\u0027s a WLB? Worker?\n\n\n\n\n\u003e \n\u003e Depending on how many many containers you want in each ASG, you may want different number of instances.\n\u003e \n\u003e \u003e There are 3 different ASG that could potentially be tuned with [min-desired-max], one for each ASG.\n\u003e \u003e \n\u003e \u003e * haproxy -\u003e this should go to a 2-2-n (as per your point in the previous CR, I will raise the relevant change)\n\u003e \n\u003e N should be configurable\n\n\n\nYes agreed, I\u0027ll raise an issue for this.\n\n\n\n\n\u003e \n\u003e \u003e * master -\u003e this should be 1-1-1, always, because masters cannot scale.\n\u003e \n\u003e Nope, you would need extra horsepower for additional components, as I mentioned.\n\n\nI believe the extra components are there already (if you mean the ones for replicas). If not, which other components do you mean?\n\n\n\u003e Also, when you do blue/green deployments, I imagine that ECS would do:\n\u003e a. Create the new task\n\u003e b. Wait for the task to be healthy\n\u003e c. Kill the old task\n\u003e \n\u003e If you have only 1 fixed EC2 instance, it may actually fail to do the upgrades with blue/green deployments.\n\u003e \n\u003e \u003e * replica -\u003e this should be 1-1-1 until we introduce EFS that allow them to scale.\n\u003e \n\u003e Why? Replicas are intended to be scaled up, as they are stateless. This is exactly the point where we want to have the maximum number of instances as a parameter.\n\u003e \n\u003e We do have already the parameter for allowing them to scale up, I can understand the point of making it 3 parameters, which is fair. I don\u0027t get the point of hardcoding the maximum values, as they are not hardcoded now.\n\n\n\n\n\nLet\u0027s say we scaled replicas to 3 instances.\nNow we have a NLB, with a targetGroup containing 3 replicas running on 3 different EC2 instances, each one having its own independent git data (on EBS).\n\nWhen masters replicate to the replica NLB, only _one_ of the 3 will receive the update, leaving the other ones behind.\n\nWhat\u0027s the point of increasing the number of replicas before we have an EFS to share across them?\n\n\n[1] https://aws.amazon.com/about-aws/whats-new/2018/06/amazon-ecs-adds-daemon-scheduling/",
      "parentUuid": "7372e1b8_b8275fe1",
      "revId": "cdb22d46accf5c11956d0202541526f5dbb4e2d4",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "90ff421e_43ed4ef0",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 6
      },
      "lineNbr": 0,
      "author": {
        "id": 1006192
      },
      "writtenOn": "2020-11-02T12:39:48Z",
      "side": 1,
      "message": "\u003e \u003e \u003e \u003e If that was the case, why we didn\u0027t have a fixed number of EC2 instances before?\n\u003e \u003e \u003e \n\u003e \u003e \u003e Because depending on the spec of Gerrit you might have wanted/needed more EC2 instances to host them:\n\u003e \u003e \u003e - If you deployed masters with very low ram/cpu needs, for example, then perhaps 2 instances would have been enough (to host masters,haproxy,slaves), whilst if you needed more resources for Gerrit, you might have wanted more ec2 instances to allow them to be allocated.\n\u003e \u003e \n\u003e \u003e I don\u0027t get it. If you had two masters, how allocating 6 EC2 would have given more resources?\n\u003e \n\u003e \n\u003e \n\u003e Imagine these are the specs defined for your deployment\n\u003e \n\u003e * Master Service *\n\u003e Gerrit:\n\u003e - Memory: 6Gb\n\u003e - Cpu: 1024 (1 vCPU)\n\u003e \n\u003e * Replica Service *\n\u003e Gerrit:\n\u003e - Memory: 6Gb\n\u003e - Cpu: 1024 (1 vCPU)\n\u003e \n\u003e Git daemon:\n\u003e - Memory: 512Mb\n\u003e - Cpu: 256 (0.25 vCPU)\n\u003e \n\u003e Git SSH:\n\u003e - Memory: 512Mb\n\u003e - Cpu: 256 (0.25 vCPU)\n\u003e \n\u003e * Loadbalancer service *\n\u003e \n\u003e ha-proxy:\n\u003e - Memory: 2Gb\n\u003e - Cpu: 1024 (1 vCPU)\n\u003e \n\u003e Total Memory needed: (6Gb x 2) + (6Gb + 512Mb + 512Mb) + 2Gb \u003d 19Gb\n\u003e Total CPU needed:   (1024 x 2) + (1024 + 256 + 256)    + 1024 \u003d 4608 (~ 5vCPUs)\n\u003e \n\u003e Now, let\u0027s assume your you have chosen a cluster EC2 instance type of\n\u003e \n\u003e m4.xlarge: 4 vCPUs, 16 Gb RAM\n\u003e \n\u003e You will need a desired number of instances of _at least_ \u00272\u0027: if you had \u00271\u0027 you wouldn\u0027t have room for 19Gb and 5 vCPUs.\n\nIf you needed resources for 5 containers, how 6 instances could have helped out?\nI do not believe AWS can split a container across multiple EC2 instances, to provide more resources.\n\n\u003e If you changed your gerrit specs to be higher however, you might need a cluster of 3, and so on.\n\u003e \n\u003e The number of instances in the cluster is a function of _needed resources_, not just of _number of instances_.\n\nI am not 100% convinced AWS can do that, also inside an ECS cluster.\nAt the end of the day, ECS is just an orchestrator of Docker containers, and the minimum allocation is 1 container per instance, but won\u0027t be able to go beyond that. The maximum allocation instead would be all containers in a single instance.\n\n\u003e \u003e I believe before it was more forward thinking, allowing the next iterations of this recipe to have more horsepower. Remember that we still need to add additional components:\n\u003e \u003e - Git daemon\n\u003e \u003e - Git/SSH\n\u003e ^^^ These are already part of the replica service\n\nAck. So the replica service should have a number of configurable instances, correct? With a maximum of 2.\n\n\u003e \u003e - JGit GC node\n\u003e \n\u003e Not sure this one will need to be a component: it is an operational task and it could be modelled as a ECS daemon[1], or a step function, or SSM, or a lambda, etc.\n\nAn ECS daemon would still need to be allocated in the cluster with some placement, correct?\n\nAn AWS step function is just an orchestration of something, not really a computing entity whilst with the AWS lambdas, I\u0027m not 100% sure they are designed to run a long, CPU and memory intensive operation, like the JGit GC.\n\n\u003e \u003e \n\u003e \u003e \u003e \u003e Also, the ASG is for the EC2 instances of the ECS cluster, and not for *exclusively* the master node.\n\u003e \u003e \u003e \n\u003e \u003e \u003e Not anymore, we have 3 ASGs in the cluster at the moment (master, replica, haproxy) and the \"desired number of instances\" is not a property of the ECS cluster, but rather a property of the ASG.\n\u003e \u003e \n\u003e \u003e Aren\u0027t the 3 ASGs part of the ECS cluster, just with different labels?\n\u003e \n\u003e Yes, they are 3 ASGs _registered_ to the same ECS cluster.\n\u003e \n\u003e \u003e We can of course have 2 settings now:\n\u003e \u003e - Desired number of masters\u0027 EC2 instances\n\u003e \u003e - Desired number of WLB EC2 instances\n\u003e \n\u003e What\u0027s a WLB? Worker?\n\nWLB \u003d Workload Balancer \u003d HA Proxy\n\n\u003e \u003e Depending on how many many containers you want in each ASG, you may want different number of instances.\n\u003e \u003e \n\u003e \u003e \u003e There are 3 different ASG that could potentially be tuned with [min-desired-max], one for each ASG.\n\u003e \u003e \u003e \n\u003e \u003e \u003e * haproxy -\u003e this should go to a 2-2-n (as per your point in the previous CR, I will raise the relevant change)\n\u003e \u003e \n\u003e \u003e N should be configurable\n\u003e \n\u003e Yes agreed, I\u0027ll raise an issue for this.\n\n+1\n\n\u003e \u003e \n\u003e \u003e \u003e * master -\u003e this should be 1-1-1, always, because masters cannot scale.\n\u003e \u003e \n\u003e \u003e Nope, you would need extra horsepower for additional components, as I mentioned.\n\u003e \n\u003e \n\u003e I believe the extra components are there already (if you mean the ones for replicas). If not, which other components do you mean?\n\nE.g. I am not sure how would you do upgrades without having the ability to scale, isn\u0027t it? Or does ECS already increase the ASG to 2 for upgrades? Also, see the JGit GC discussion, that would need some node allocation somewhere anyway.\n\n\u003e Let\u0027s say we scaled replicas to 3 instances.\n\u003e Now we have a NLB, with a targetGroup containing 3 replicas running on 3 different EC2 instances, each one having its own independent git data (on EBS).\n\u003e \n\u003e When masters replicate to the replica NLB, only _one_ of the 3 will receive the update, leaving the other ones behind.\n\nNope, when you have replicas with ASG they need to share the repositories, otherwise when they scale up then won\u0027t have the latest version of the repositories.\n\n\u003e What\u0027s the point of increasing the number of replicas before we have an EFS to share across them?\n\nHorizontal scalability, for managing peaks of incoming traffic.\n\n\u003e [1] https://aws.amazon.com/about-aws/whats-new/2018/06/amazon-ecs-adds-daemon-scheduling/",
      "parentUuid": "eb656d7e_2a17c3bb",
      "revId": "cdb22d46accf5c11956d0202541526f5dbb4e2d4",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "9cef0b1d_9e8c529e",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 6
      },
      "lineNbr": 0,
      "author": {
        "id": 1072905
      },
      "writtenOn": "2020-11-02T13:47:13Z",
      "side": 1,
      "message": "Hey Luca, sorry for top posting here, but the conversation is getting quite nested and I start struggling to find the inline replies, feel free to re-compact this if you feel like.\n\nMeanwhile, my replies in-line ðŸ˜Š\n\n\n\n\u003e \u003e \u003e \u003e \u003e If that was the case, why we didn\u0027t have a fixed number of EC2 instances before?\n\u003e \u003e \u003e \u003e \n\u003e \u003e \u003e \u003e Because depending on the spec of Gerrit you might have wanted/needed more EC2 instances to host them:\n\u003e \u003e \u003e \u003e - If you deployed masters with very low ram/cpu needs, for example, then perhaps 2 instances would have been enough (to host masters,haproxy,slaves), whilst if you needed more resources for Gerrit, you might have wanted more ec2 instances to allow them to be allocated.\n\u003e \u003e \u003e \n\u003e \u003e \u003e I don\u0027t get it. If you had two masters, how allocating 6 EC2 would have given more resources?\n\u003e \u003e \n\u003e \u003e \n\u003e \u003e \n\u003e \u003e Imagine these are the specs defined for your deployment\n\u003e \u003e \n\u003e \u003e * Master Service *\n\u003e \u003e Gerrit:\n\u003e \u003e - Memory: 6Gb\n\u003e \u003e - Cpu: 1024 (1 vCPU)\n\u003e \u003e \n\u003e \u003e * Replica Service *\n\u003e \u003e Gerrit:\n\u003e \u003e - Memory: 6Gb\n\u003e \u003e - Cpu: 1024 (1 vCPU)\n\u003e \u003e \n\u003e \u003e Git daemon:\n\u003e \u003e - Memory: 512Mb\n\u003e \u003e - Cpu: 256 (0.25 vCPU)\n\u003e \u003e \n\u003e \u003e Git SSH:\n\u003e \u003e - Memory: 512Mb\n\u003e \u003e - Cpu: 256 (0.25 vCPU)\n\u003e \u003e \n\u003e \u003e * Loadbalancer service *\n\u003e \u003e \n\u003e \u003e ha-proxy:\n\u003e \u003e - Memory: 2Gb\n\u003e \u003e - Cpu: 1024 (1 vCPU)\n\u003e \u003e \n\u003e \u003e Total Memory needed: (6Gb x 2) + (6Gb + 512Mb + 512Mb) + 2Gb \u003d 19Gb\n\u003e \u003e Total CPU needed:   (1024 x 2) + (1024 + 256 + 256)    + 1024 \u003d 4608 (~ 5vCPUs)\n\u003e \u003e \n\u003e \u003e Now, let\u0027s assume your you have chosen a cluster EC2 instance type of\n\u003e \u003e \n\u003e \u003e m4.xlarge: 4 vCPUs, 16 Gb RAM\n\u003e \u003e \n\u003e \u003e You will need a desired number of instances of _at least_ \u00272\u0027: if you had \u00271\u0027 you wouldn\u0027t have room for 19Gb and 5 vCPUs.\n\u003e \n\u003e If you needed resources for 5 containers, how 6 instances could have helped out?\n\n\n\n\n\nThey wouldn\u0027t, of course, because you have EC2 instances that you would never be able to use (unless you start scaling).\nBut you were able to decide if you wanted those 5 instances over 3 m4.xlarge or over 2 m4.2xlarge, for example.\n\n\n\n\n\u003e I do not believe AWS can split a container across multiple EC2 instances, to provide more resources.\n\n\n\n\nThe most granular component that can be deployed is not the container, it\u0027s the task.\n\n* ECS Instance -\u003e EC2 instance with an ecs-agent running on it.\n* Service -\u003e collection of Tasks\n* Task -\u003e collection of Containers (replica task for example is composed by gerrit/ssh/daemon)\n\nWhilst the service can span across multiple ECS Instances, the Task cannot and it is always deployed in the same ECS Instance.\n\n\n\n\n\n\n\n\u003e \n\u003e \u003e If you changed your gerrit specs to be higher however, you might need a cluster of 3, and so on.\n\u003e \u003e \n\u003e \u003e The number of instances in the cluster is a function of _needed resources_, not just of _number of instances_.\n\u003e \n\u003e I am not 100% convinced AWS can do that, also inside an ECS cluster.\n\u003e At the end of the day, ECS is just an orchestrator of Docker containers, and the minimum allocation is 1 container per instance, but won\u0027t be able to go beyond that. The maximum allocation instead would be all containers in a single instance.\n\n\n\n\nThe minimum allocation is 1 _Task_ per instance, not 1 container.\nThe most compact allocation is all _Services_ in a single instance (but as we have seen we cannot have this topology because tasks wouldn\u0027t be able to talk to each others via the NLB).\n\n\n\n\n\u003e \n\u003e \u003e \u003e I believe before it was more forward thinking, allowing the next iterations of this recipe to have more horsepower. Remember that we still need to add additional components:\n\u003e \u003e \u003e - Git daemon\n\u003e \u003e \u003e - Git/SSH\n\u003e \u003e ^^^ These are already part of the replica service\n\u003e \n\u003e Ack. So the replica service should have a number of configurable instances, correct? With a maximum of 2.\n\u003e \n\u003e \u003e \u003e - JGit GC node\n\u003e \u003e \n\u003e \u003e Not sure this one will need to be a component: it is an operational task and it could be modelled as a ECS daemon[1], or a step function, or SSM, or a lambda, etc.\n\u003e \n\u003e An ECS daemon would still need to be allocated in the cluster with some placement, correct?\n\u003e \n\u003e An AWS step function is just an orchestration of something, not really a computing entity whilst with the AWS lambdas, I\u0027m not 100% sure they are designed to run a long, CPU and memory intensive operation, like the JGit GC.\n\n\n\n\n\nAs I was mentioning, I am not sure, I think there are different approaches, some of which do not need any additional allocation (SSM agent[1] already runs on Amazon Linux images, so it could be \"talked to\" without running extra components).\nShall we discuss/brainstorm about this specific issue in the relevant issue ticket?[2]\n\n\n\u003e \n\u003e \u003e \u003e \n\u003e \u003e \u003e \u003e \u003e Also, the ASG is for the EC2 instances of the ECS cluster, and not for *exclusively* the master node.\n\u003e \u003e \u003e \u003e \n\u003e \u003e \u003e \u003e Not anymore, we have 3 ASGs in the cluster at the moment (master, replica, haproxy) and the \"desired number of instances\" is not a property of the ECS cluster, but rather a property of the ASG.\n\u003e \u003e \u003e \n\u003e \u003e \u003e Aren\u0027t the 3 ASGs part of the ECS cluster, just with different labels?\n\u003e \u003e \n\u003e \u003e Yes, they are 3 ASGs _registered_ to the same ECS cluster.\n\u003e \u003e \n\u003e \u003e \u003e We can of course have 2 settings now:\n\u003e \u003e \u003e - Desired number of masters\u0027 EC2 instances\n\u003e \u003e \u003e - Desired number of WLB EC2 instances\n\u003e \u003e \n\u003e \u003e What\u0027s a WLB? Worker?\n\u003e \n\u003e WLB \u003d Workload Balancer \u003d HA Proxy\n\n+1\n\n\u003e \n\u003e \u003e \u003e Depending on how many many containers you want in each ASG, you may want different number of instances.\n\u003e \u003e \u003e \n\u003e \u003e \u003e \u003e There are 3 different ASG that could potentially be tuned with [min-desired-max], one for each ASG.\n\u003e \u003e \u003e \u003e \n\u003e \u003e \u003e \u003e * haproxy -\u003e this should go to a 2-2-n (as per your point in the previous CR, I will raise the relevant change)\n\u003e \u003e \u003e \n\u003e \u003e \u003e N should be configurable\n\u003e \u003e \n\u003e \u003e Yes agreed, I\u0027ll raise an issue for this.\n\u003e \n\u003e +1\n\n\nDone: https://gerrit-review.googlesource.com/c/aws-gerrit/+/286280\n\n\u003e \n\u003e \u003e \u003e \n\u003e \u003e \u003e \u003e * master -\u003e this should be 1-1-1, always, because masters cannot scale.\n\u003e \u003e \u003e \n\u003e \u003e \u003e Nope, you would need extra horsepower for additional components, as I mentioned.\n\u003e \u003e \n\u003e \u003e \n\u003e \u003e I believe the extra components are there already (if you mean the ones for replicas). If not, which other components do you mean?\n\u003e \n\u003e E.g. I am not sure how would you do upgrades without having the ability to scale, isn\u0027t it? Or does ECS already increase the ASG to 2 for upgrades? Also, see the JGit GC discussion, that would need some node allocation somewhere anyway.\n\u003e \n\n\n\n\n\n\n\nScaling for \"more horsepower\" for masters is something I don\u0027t get, masters only run gerrit masters and I am not convinced we need to run anything else alongside them (at least for now).\n\nRolling upgrades is an interesting subject and you\u0027re right, it might fall into the same conversation.\n\nTBH, I was thinking something along the lines of\n- deploy a completely new green cluster (maintaining the data)\n- Switch traffic to green\n- delete blue cluster\n\nWere you thinking about scaling up the blue cluster and let old and new co-living together in the same cluster?\n\nP.S. Should I create a monorail ticket to address upgrades?\n\n\n\n\n\n\n\n\u003e \u003e Let\u0027s say we scaled replicas to 3 instances.\n\u003e \u003e Now we have a NLB, with a targetGroup containing 3 replicas running on 3 different EC2 instances, each one having its own independent git data (on EBS).\n\u003e \u003e \n\u003e \u003e When masters replicate to the replica NLB, only _one_ of the 3 will receive the update, leaving the other ones behind.\n\u003e \n\u003e Nope, when you have replicas with ASG they need to share the repositories, otherwise when they scale up then won\u0027t have the latest version of the repositories.\n\n\n\n\n\n\nExactly, they need to share the repositories, but they just don\u0027t do that at the moment.\nThat\u0027s why I was asking \"What\u0027s the point of increasing the number of replicas _before we have an EFS to share across them?_\"\n\nI understand the benefit of scaling replicas _once_ they share repositories.\n\n\n\n\n\u003e \n\u003e \u003e What\u0027s the point of increasing the number of replicas before we have an EFS to share across them?\n\u003e \n\u003e Horizontal scalability, for managing peaks of incoming traffic.\n\n\n\n\nI understand why we need to scale replicas, of course.\nBut I believe the requirement for this is for replicas to share the repositories[3] first. Otherwise we would be scaling replicas that \"won\u0027t have the latest version of the repositories\", as you put it.\n\n\n\n\n\u003e \n\u003e \u003e [1] https://aws.amazon.com/about-aws/whats-new/2018/06/amazon-ecs-adds-daemon-scheduling/\n\n\n\n[1]https://docs.aws.amazon.com/systems-manager/latest/userguide/ssm-agent.html\n[2]https://bugs.chromium.org/p/gerrit/issues/detail?id\u003d13620\n[3]https://bugs.chromium.org/p/gerrit/issues/detail?id\u003d13619",
      "parentUuid": "90ff421e_43ed4ef0",
      "revId": "cdb22d46accf5c11956d0202541526f5dbb4e2d4",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "e5212982_e982cc2d",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 6
      },
      "lineNbr": 0,
      "author": {
        "id": 1006192
      },
      "writtenOn": "2020-11-04T11:10:12Z",
      "side": 1,
      "message": "Compacting, by leaving only the Q\u0026As:\n\n\u003e \u003e If you needed resources for 5 containers, how 6 instances could have helped out?\n\u003e \n\u003e They wouldn\u0027t, of course, because you have EC2 instances that you would never be able to use (unless you start scaling).\n\nOK, so the 6 instances we had before were unneeded? or would have been useful to manage some spare capacity in case of restarts/upgrades?\n\n\u003e But you were able to decide if you wanted those 5 instances over 3 m4.xlarge or over 2 m4.2xlarge, for example.\n\nI don\u0027t believe we had the ability to mix instance types, but we had only one instance type parameter. But yes, you could have opted for less instances and more powerful instances types vs. more instances with less power.\n\nAnyway, I see now the limit of not being able to configure the number of instances anymore, which is a minus IMHO.\n\n\u003e \u003e I do not believe AWS can split a container across multiple EC2 instances, to provide more resources.\n\u003e \n\u003e The most granular component that can be deployed is not the container, it\u0027s the task.\n\u003e \n\u003e * ECS Instance -\u003e EC2 instance with an ecs-agent running on it.\n\u003e * Service -\u003e collection of Tasks\n\u003e * Task -\u003e collection of Containers (replica task for example is composed by gerrit/ssh/daemon)\n\nOh, that\u0027s not good then. You may actually want to have gerrit, ssh and the daemon potentially on different EC2 instances. I believe that is something to address as a follow-up.\n\n\u003e Whilst the service can span across multiple ECS Instances, the Task cannot and it is always deployed in the same ECS Instance.\n\nThanks for clarifying: that highlights the problem that the Git/SSH and Git/daemon would then impact on the Gerrit resources, which is bad.\nAs mentioned above, it can be addressed as a follow-up change.\n\n\u003e \u003e \u003e If you changed your gerrit specs to be higher however, you might need a cluster of 3, and so on.\n\u003e \u003e \u003e \n\u003e \u003e \u003e The number of instances in the cluster is a function of _needed resources_, not just of _number of instances_.\n\u003e \u003e \n\u003e \u003e I am not 100% convinced AWS can do that, also inside an ECS cluster.\n\u003e \u003e At the end of the day, ECS is just an orchestrator of Docker containers, and the minimum allocation is 1 container per instance, but won\u0027t be able to go beyond that. The maximum allocation instead would be all containers in a single instance.\n\u003e \n\u003e The minimum allocation is 1 _Task_ per instance, not 1 container.\n\u003e The most compact allocation is all _Services_ in a single instance (but as we have seen we cannot have this topology because tasks wouldn\u0027t be able to talk to each others via the NLB).\n\nSure, but my point is that if you have 1 task that requires more resources than the EC2 instance type you have, then having more instances won\u0027t help.\n\nIs that understanding correct?\n\n\u003e \u003e An ECS daemon would still need to be allocated in the cluster with some placement, correct?\n\u003e \u003e \n\u003e \u003e An AWS step function is just an orchestration of something, not really a computing entity whilst with the AWS lambdas, I\u0027m not 100% sure they are designed to run a long, CPU and memory intensive operation, like the JGit GC.\n\u003e \n\u003e As I was mentioning, I am not sure, I think there are different approaches, some of which do not need any additional allocation (SSM agent[1] already runs on Amazon Linux images, so it could be \"talked to\" without running extra components).\n\u003e Shall we discuss/brainstorm about this specific issue in the relevant issue ticket?[2]\n\nAgreed, we can take this discussion off-line.\n\nWe can just focus on the issue we need to address as part of this change: the regression on the number of instances, before configurable and now hardcoded to 1.\n\n\u003e Scaling for \"more horsepower\" for masters is something I don\u0027t get, masters only run gerrit masters and I am not convinced we need to run anything else alongside them (at least for now).\n\u003e \n\u003e Rolling upgrades is an interesting subject and you\u0027re right, it might fall into the same conversation.\n\u003e \n\u003e TBH, I was thinking something along the lines of\n\u003e - deploy a completely new green cluster (maintaining the data)\n\u003e - Switch traffic to green\n\u003e - delete blue cluster\n\nThat looks quite an overkill IMHO: ECS already manages blue/green deployments automatically. What I don\u0027t know is *IF* the maximum number of instances in the cluster needs to accommodate that or not.\n\n\u003e Were you thinking about scaling up the blue cluster and let old and new co-living together in the same cluster?\n\u003e \n\u003e P.S. Should I create a monorail ticket to address upgrades?\n\nNope, this is something ECS should do, not us. We don\u0027t want to reinvent the wheel, but integrate as much as possible with the native AWS ECS features.\n\nIf we don\u0027t use them, it\u0027s fine but we need to have a strong argument behind it.\n\n\u003e \u003e Nope, when you have replicas with ASG they need to share the repositories, otherwise when they scale up then won\u0027t have the latest version of the repositories.\n\u003e \n\u003e Exactly, they need to share the repositories, but they just don\u0027t do that at the moment.\n\u003e That\u0027s why I was asking \"What\u0027s the point of increasing the number of replicas _before we have an EFS to share across them?_\"\n\u003e \n\u003e I understand the benefit of scaling replicas _once_ they share repositories.\n\nSure, but my point is: what the point of removing the parameter for adding it back again tomorrow?\n\n\u003e \u003e \u003e What\u0027s the point of increasing the number of replicas before we have an EFS to share across them?\n\u003e \u003e \n\u003e \u003e Horizontal scalability, for managing peaks of incoming traffic.\n\u003e \n\u003e I understand why we need to scale replicas, of course.\n\u003e But I believe the requirement for this is for replicas to share the repositories[3] first. Otherwise we would be scaling replicas that \"won\u0027t have the latest version of the repositories\", as you put it.\n\nAck, but please see also my point of not removing something that you add back immediately afterwards.\n\n\u003e [1]https://docs.aws.amazon.com/systems-manager/latest/userguide/ssm-agent.html\n\u003e [2]https://bugs.chromium.org/p/gerrit/issues/detail?id\u003d13620\n\u003e [3]https://bugs.chromium.org/p/gerrit/issues/detail?id\u003d13619",
      "parentUuid": "9cef0b1d_9e8c529e",
      "revId": "cdb22d46accf5c11956d0202541526f5dbb4e2d4",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "64eb7a92_120a44d5",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 6
      },
      "lineNbr": 0,
      "author": {
        "id": 1072905
      },
      "writtenOn": "2020-11-04T14:11:05Z",
      "side": 1,
      "message": "\u003e Compacting, by leaving only the Q\u0026As:\n\u003e \n\u003e \u003e \u003e If you needed resources for 5 containers, how 6 instances could have helped out?\n\u003e \u003e \n\u003e \u003e They wouldn\u0027t, of course, because you have EC2 instances that you would never be able to use (unless you start scaling).\n\u003e \n\u003e OK, so the 6 instances we had before were unneeded? or would have been useful to manage some spare capacity in case of restarts/upgrades?\n\n\nI think they were needed to accommodate the wanted resources across the chosen instanceType:\n- raising RAM and CPU might have required to have more EC2 of type $instance-type.\n- increasing $instance-type might have allowed to have fewer EC2 to run the cluster.\n\nPerhaps restarts/upgrades will require to scale elastically (depending on how we decide to do it), but scaling masters ASG alone doesn\u0027t just give us the ability to upgrade.\n\nIf you think allowing master ASGs to scale will be useful as a preparation for upgrade strategies then we do so, but I don\u0027t have it clear in my mind how those upgrades will be carried out yet, so I am not sure if that is going to help.\n\nI do believe we need a ticket to capture what\u0027s required to perform cluster upgrades (more on this later).\n\n\n\n\n\u003e \n\u003e \u003e But you were able to decide if you wanted those 5 instances over 3 m4.xlarge or over 2 m4.2xlarge, for example.\n\u003e \n\u003e I don\u0027t believe we had the ability to mix instance types, but we had only one instance type parameter. But yes, you could have opted for less instances and more powerful instances types vs. more instances with less power.\n\u003e \n\u003e Anyway, I see now the limit of not being able to configure the number of \ninstances anymore, which is a minus IMHO.\n\nThese are the components we have in dual-master: master, haproxy, replica.\nThey cannot share the same EC2 instance, otherwise failing to connect to each other (that\u0027s why we separated replicas and haproxy into their own ASG and why now we are doing this with masters).\n\nAs a consequence of the above we now have 3 separate ASGs.\n\nIn the past everything was in the same ASG, so it made sense to be able to scale (because even if only one component needed, then the entire ASG needed to scale), but now that we have separate ASG we should decide which ones should scale and which ones shouldn\u0027t.\n\n- haproxy: it is already able to scale up up to a configurable number.\n- replicas: should be able to scale up, but it is not useful until we have an EFS, so we should do that first.\n- master: I don\u0027t see what it would mean to allow to have 3 or 4 master1s. If it\u0027s just for upgrading then we might need to *temporarily* allow scaling (up to 2 only?), but I think this should be dealt with when implementing cluster upgrades policies.\n\n\n\n\n\n\n\n\n\n\u003e \n\u003e \u003e \u003e I do not believe AWS can split a container across multiple EC2 instances, to provide more resources.\n\u003e \u003e \n\u003e \u003e The most granular component that can be deployed is not the container, it\u0027s the task.\n\u003e \u003e \n\u003e \u003e * ECS Instance -\u003e EC2 instance with an ecs-agent running on it.\n\u003e \u003e * Service -\u003e collection of Tasks\n\u003e \u003e * Task -\u003e collection of Containers (replica task for example is composed by gerrit/ssh/daemon)\n\u003e \n\u003e Oh, that\u0027s not good then. You may actually want to have gerrit, ssh and the daemon potentially on different EC2 instances. I believe that is something to address as a follow-up.\n\nHow can you separate the Git and SSH Daemon from the replica?\nGit and SSH daemons are the only means for which replication data can get to the replica EBS volume, isn\u0027t?\n\n\n\n\n\n\n\n\n\u003e \n\u003e \u003e Whilst the service can span across multiple ECS Instances, the Task cannot and it is always deployed in the same ECS Instance.\n\u003e \n\u003e Thanks for clarifying: that highlights the problem that the Git/SSH and Git/daemon would then impact on the Gerrit resources, which is bad.\n\u003e As mentioned above, it can be addressed as a follow-up change.\n\n\nSee above, how can you separate them from the replica?\n\n\n\n\n\n\n\n\n\u003e \n\u003e \u003e \u003e \u003e If you changed your gerrit specs to be higher however, you might need a cluster of 3, and so on.\n\u003e \u003e \u003e \u003e \n\u003e \u003e \u003e \u003e The number of instances in the cluster is a function of _needed resources_, not just of _number of instances_.\n\u003e \u003e \u003e \n\u003e \u003e \u003e I am not 100% convinced AWS can do that, also inside an ECS cluster.\n\u003e \u003e \u003e At the end of the day, ECS is just an orchestrator of Docker containers, and the minimum allocation is 1 container per instance, but won\u0027t be able to go beyond that. The maximum allocation instead would be all containers in a single instance.\n\u003e \u003e \n\u003e \u003e The minimum allocation is 1 _Task_ per instance, not 1 container.\n\u003e \u003e The most compact allocation is all _Services_ in a single instance (but as we have seen we cannot have this topology because tasks wouldn\u0027t be able to talk to each others via the NLB).\n\u003e \n\u003e Sure, but my point is that if you have 1 task that requires more resources than the EC2 instance type you have, then having more instances won\u0027t help.\n\u003e \n\u003e Is that understanding correct?\n\nYes, if you wanted to deploy one task that required more resources than the EC2 instance type, you will need to increase the instance-type to allow that task to be deployed.\n\nBut this is always the case right? you need to run the task on an EC2 instance that _can_ run it.\n\n\n\n\n\n\n\u003e \n\u003e \u003e \u003e An ECS daemon would still need to be allocated in the cluster with some placement, correct?\n\u003e \u003e \u003e \n\u003e \u003e \u003e An AWS step function is just an orchestration of something, not really a computing entity whilst with the AWS lambdas, I\u0027m not 100% sure they are designed to run a long, CPU and memory intensive operation, like the JGit GC.\n\u003e \u003e \n\u003e \u003e As I was mentioning, I am not sure, I think there are different approaches, some of which do not need any additional allocation (SSM agent[1] already runs on Amazon Linux images, so it could be \"talked to\" without running extra components).\n\u003e \u003e Shall we discuss/brainstorm about this specific issue in the relevant issue ticket?[2]\n\u003e \n\u003e Agreed, we can take this discussion off-line.\n\u003e \n\u003e We can just focus on the issue we need to address as part of this change: the regression on the number of instances, before configurable and now hardcoded to 1.\n\u003e \n\nSee my reply above, I believe we can still configure what is needed (i.e. haproxy, replicas in the future). We never needed to have master on autoscaling groups greater than 1, imho.\n\n\n\n\n\n\u003e \u003e Scaling for \"more horsepower\" for masters is something I don\u0027t get, masters only run gerrit masters and I am not convinced we need to run anything else alongside them (at least for now).\n\u003e \u003e \n\u003e \u003e Rolling upgrades is an interesting subject and you\u0027re right, it might fall into the same conversation.\n\u003e \u003e \n\u003e \u003e TBH, I was thinking something along the lines of\n\u003e \u003e - deploy a completely new green cluster (maintaining the data)\n\u003e \u003e - Switch traffic to green\n\u003e \u003e - delete blue cluster\n\u003e \n\u003e That looks quite an overkill IMHO: ECS already manages blue/green deployments automatically. What I don\u0027t know is *IF* the maximum number of instances in the cluster needs to accommodate that or not.\n\u003e \n\u003e \u003e Were you thinking about scaling up the blue cluster and let old and new co-living together in the same cluster?\n\u003e \u003e \n\u003e \u003e P.S. Should I create a monorail ticket to address upgrades?\n\u003e \n\u003e Nope, this is something ECS should do, not us. We don\u0027t want to reinvent the wheel, but integrate as much as possible with the native AWS ECS features.\n\u003e \n\u003e If we don\u0027t use them, it\u0027s fine but we need to have a strong argument behind it.\n\nI think we should use ECS to do blue/green deployments, absolutely.\nBut I believe, that this requires some work:\n\nSome examples:\n- there\u0027s no \"make upgrade\" in aws gerrit. you can only trigger a creation or deletion.\n- If you were to upgrade loadbalancers you would need to point route53 to them (or add/remove listeners)\n- We would need to drain connection and then switch the DNS from green to blue.\n- ...\n\nIf you wanted to trigger a cluster update today how would you do it? \n \n\n\n\n\n\n\u003e \n\u003e \u003e \u003e Nope, when you have replicas with ASG they need to share the repositories, otherwise when they scale up then won\u0027t have the latest version of the repositories.\n\u003e \u003e \n\u003e \u003e Exactly, they need to share the repositories, but they just don\u0027t do that at the moment.\n\u003e \u003e That\u0027s why I was asking \"What\u0027s the point of increasing the number of replicas _before we have an EFS to share across them?_\"\n\u003e \u003e \n\u003e \u003e I understand the benefit of scaling replicas _once_ they share repositories.\n\u003e \n\u003e Sure, but my point is: what the point of removing the parameter for adding it back again tomorrow?\n\n\nThis CR is not removing any scaling abilities from replicas.\nThat\u0027s already set at 1, from the previous change we merged, which I think was the correct thing to do, given they don\u0027t share any EFS.\n\n\n\n\n\n\u003e \n\u003e \u003e \u003e \u003e What\u0027s the point of increasing the number of replicas before we have an EFS to share across them?\n\u003e \u003e \u003e \n\u003e \u003e \u003e Horizontal scalability, for managing peaks of incoming traffic.\n\u003e \u003e \n\u003e \u003e I understand why we need to scale replicas, of course.\n\u003e \u003e But I believe the requirement for this is for replicas to share the repositories[3] first. Otherwise we would be scaling replicas that \"won\u0027t have the latest version of the repositories\", as you put it.\n\u003e \n\u003e Ack, but please see also my point of not removing something that you add back immediately afterwards.\n\nWe don\u0027t remove scalability from replicas in this CR.\n\n\n\n\n\n\n\u003e \n\u003e \u003e [1]https://docs.aws.amazon.com/systems-manager/latest/userguide/ssm-agent.html\n\u003e \u003e [2]https://bugs.chromium.org/p/gerrit/issues/detail?id\u003d13620\n\u003e \u003e [3]https://bugs.chromium.org/p/gerrit/issues/detail?id\u003d13619",
      "parentUuid": "e5212982_e982cc2d",
      "revId": "cdb22d46accf5c11956d0202541526f5dbb4e2d4",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "349b6ca9_de21b145",
        "filename": "common-templates/cf-master-asg.yml",
        "patchSetId": 6
      },
      "lineNbr": 54,
      "author": {
        "id": 1006192
      },
      "writtenOn": "2020-10-29T15:59:21Z",
      "side": 1,
      "message": "Should this be a parameter of the template?\n\nP.S. It was before a 1 to 6 ASG, whilst now it is just 1 instance.",
      "range": {
        "startLine": 52,
        "startChar": 0,
        "endLine": 54,
        "endChar": 26
      },
      "revId": "cdb22d46accf5c11956d0202541526f5dbb4e2d4",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "f956ee3e_49aca5cb",
        "filename": "common-templates/cf-master-asg.yml",
        "patchSetId": 6
      },
      "lineNbr": 54,
      "author": {
        "id": 1072905
      },
      "writtenOn": "2020-10-29T16:48:09Z",
      "side": 1,
      "message": "See my reply above.",
      "parentUuid": "349b6ca9_de21b145",
      "range": {
        "startLine": 52,
        "startChar": 0,
        "endLine": 54,
        "endChar": 26
      },
      "revId": "cdb22d46accf5c11956d0202541526f5dbb4e2d4",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "12db68ce_39cc55b4",
        "filename": "common-templates/cf-master-asg.yml",
        "patchSetId": 6
      },
      "lineNbr": 54,
      "author": {
        "id": 1006192
      },
      "writtenOn": "2020-10-29T17:16:35Z",
      "side": 1,
      "message": "Answered.",
      "parentUuid": "f956ee3e_49aca5cb",
      "range": {
        "startLine": 52,
        "startChar": 0,
        "endLine": 54,
        "endChar": 26
      },
      "revId": "cdb22d46accf5c11956d0202541526f5dbb4e2d4",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "9712b62e_7512880f",
        "filename": "common-templates/cf-master-asg.yml",
        "patchSetId": 6
      },
      "lineNbr": 54,
      "author": {
        "id": 1072905
      },
      "writtenOn": "2020-10-29T17:57:33Z",
      "side": 1,
      "message": "Ack",
      "parentUuid": "12db68ce_39cc55b4",
      "range": {
        "startLine": 52,
        "startChar": 0,
        "endLine": 54,
        "endChar": 26
      },
      "revId": "cdb22d46accf5c11956d0202541526f5dbb4e2d4",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "5dd8aae3_d6213aaa",
        "filename": "common-templates/cf-master-asg.yml",
        "patchSetId": 6
      },
      "lineNbr": 54,
      "author": {
        "id": 1072905
      },
      "writtenOn": "2020-10-29T17:57:33Z",
      "side": 1,
      "message": "Ack",
      "parentUuid": "12db68ce_39cc55b4",
      "range": {
        "startLine": 52,
        "startChar": 0,
        "endLine": 54,
        "endChar": 26
      },
      "revId": "cdb22d46accf5c11956d0202541526f5dbb4e2d4",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "65cdbb2f_a9ee244e",
        "filename": "common-templates/cf-master-asg.yml",
        "patchSetId": 6
      },
      "lineNbr": 54,
      "author": {
        "id": 1006192
      },
      "writtenOn": "2020-10-30T23:37:03Z",
      "side": 1,
      "message": "Forgot to push your changes?",
      "parentUuid": "9712b62e_7512880f",
      "range": {
        "startLine": 52,
        "startChar": 0,
        "endLine": 54,
        "endChar": 26
      },
      "revId": "cdb22d46accf5c11956d0202541526f5dbb4e2d4",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "c11b44d8_c38ba9e2",
        "filename": "common-templates/cf-master-asg.yml",
        "patchSetId": 6
      },
      "lineNbr": 54,
      "author": {
        "id": 1072905
      },
      "writtenOn": "2020-11-02T10:06:23Z",
      "side": 1,
      "message": "oh yes, I had forgotten to push the commit message change, please refer to my reply for the discussion about scaling master nodes.",
      "parentUuid": "65cdbb2f_a9ee244e",
      "range": {
        "startLine": 52,
        "startChar": 0,
        "endLine": 54,
        "endChar": 26
      },
      "revId": "cdb22d46accf5c11956d0202541526f5dbb4e2d4",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "13ba35a6_4b5563cc",
        "filename": "common-templates/cf-master-asg.yml",
        "patchSetId": 6
      },
      "lineNbr": 54,
      "author": {
        "id": 1072905
      },
      "writtenOn": "2020-11-02T13:01:27Z",
      "side": 1,
      "message": "Done",
      "parentUuid": "c11b44d8_c38ba9e2",
      "range": {
        "startLine": 52,
        "startChar": 0,
        "endLine": 54,
        "endChar": 26
      },
      "revId": "cdb22d46accf5c11956d0202541526f5dbb4e2d4",
      "serverId": "173816e5-2b9a-37c3-8a2e-48639d4f1153"
    }
  ]
}